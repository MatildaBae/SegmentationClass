# -*- coding: utf-8 -*-
"""clip_face_0821.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18xQsz8IaDTWoMcrjwvGB8IhEOyI_UDBu

# 모델 불러오기
"""

import torch
from torch import nn
from transformers import AutoFeatureExtractor, AutoTokenizer, AutoModel

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# CLIP과 KoBERT 모델 불러오기
vit_model = AutoModel.from_pretrained("google/vit-base-patch16-224").to(device)
vit_processor = AutoFeatureExtractor.from_pretrained("google/vit-base-patch16-224")
kobert_model = AutoModel.from_pretrained("monologg/kobert").to(device)
kobert_tokenizer = AutoTokenizer.from_pretrained("monologg/kobert")


for param in kobert_model.parameters():
    param.requires_grad = False

# text model encoder output

text = "안녕하세요, GPT-4입니다."
text_inputs = kobert_tokenizer(text, return_tensors="pt")
text_outputs = kobert_model(**text_inputs)

text_last_hidden_state = text_outputs.last_hidden_state
text_pooled_output = text_outputs.pooler_output

print(text_last_hidden_state.shape)
print(text_pooled_output.shape)

from PIL import Image
import requests
from transformers import AutoProcessor, CLIPVisionModel

# image model encoder output
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

image_inputs = vit_processor(images=image, return_tensors="pt")

image_outputs = vit_model(**image_inputs)
image_last_hidden_state = image_outputs.last_hidden_state
image_pooled_output = image_outputs.pooler_output  # pooled CLS states

print(image_last_hidden_state.shape)
print(image_pooled_output.shape)

"""# Dataset & DataLoder"""

from google.colab import files
files.upload()  # kaggle.json 파일 업로드

# 이거 각자 kaggle account에서 Create API token해서 다운로드해서 여기 업로드한 후 진행

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Affectnet
!kaggle datasets download -d mstjebashazida/affectnet

# 압축 해제
import zipfile

with zipfile.ZipFile('affectnet.zip', 'r') as zip_ref:
    zip_ref.extractall('affectnet')

import os
import shutil

# 폴더 경로 설정
affectnet_path = '/content/affectnet/archive (3)'

# 1. Contempt 폴더 삭제
contempt_path = os.path.join(affectnet_path, 'Test', 'Contempt')
if os.path.exists(contempt_path):
    shutil.rmtree(contempt_path)

# 2. Anger 폴더 이름 변경
anger_path = os.path.join(affectnet_path, 'Test', 'Anger')
new_anger_path = os.path.join(affectnet_path, 'Test', 'anger')
os.rename(anger_path, new_anger_path)

# 3. 폴더 생성 및 이미지 이동
def move_images(src_folder, dst_folder):
    if not os.path.exists(dst_folder):
        os.makedirs(dst_folder)
    for category in ['anger', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']:
        src_category = os.path.join(src_folder, category)
        dst_category = os.path.join(dst_folder, category)
        if os.path.exists(src_category):
            if not os.path.exists(dst_category):
                os.makedirs(dst_category)
            for file_name in os.listdir(src_category):
                src_file = os.path.join(src_category, file_name)
                dst_file = os.path.join(dst_category, file_name)
                shutil.move(src_file, dst_file)

# Test 및 Train 폴더 처리
content_data_path = '/content/data'
os.makedirs(content_data_path, exist_ok=True)

for folder in ['Test', 'Train']:
    folder_path = os.path.join(affectnet_path, folder)
    move_images(folder_path, content_data_path)

# 데이터 폴더 내 이미지 개수 출력
def count_files_in_folder(folder_path):
    return sum([len(files) for r, d, files in os.walk(folder_path)])

print("Number of images in content/data:")
for category in ['anger', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']:
    folder_path = os.path.join(content_data_path, category)
    print(f"{category}: {count_files_in_folder(folder_path)}")

# FER-2013
!kaggle datasets download -d astraszab/facial-expression-dataset-image-folders-fer2013

# 압축 해제
with zipfile.ZipFile('facial-expression-dataset-image-folders-fer2013.zip', 'r') as zip_ref:
    zip_ref.extractall('fer2013')

# FER-2013 폴더 경로 설정
fer2013_path = '/content/fer2013/data'
emotion_map = {
    '0': 'anger',
    '1': 'disgust',
    '2': 'fear',
    '3': 'happy',
    '4': 'sad',
    '5': 'surprise',
    '6': 'neutral'
}

def rename_folders(base_path):
    for subset in ['test', 'train', 'val']:
        subset_path = os.path.join(base_path, subset)
        for num, emotion in emotion_map.items():
            num_folder = os.path.join(subset_path, num)
            emotion_folder = os.path.join(subset_path, emotion)
            if os.path.exists(num_folder):
                os.rename(num_folder, emotion_folder)

def move_fer2013_images(base_path, target_path):
    for subset in ['test', 'train', 'val']:
        subset_path = os.path.join(base_path, subset)
        move_images(subset_path, target_path)

# 폴더 이름 변경
rename_folders(fer2013_path)

# FER-2013 이미지를 content/data로 이동
move_fer2013_images(fer2013_path, content_data_path)

# 데이터 폴더 내 이미지 개수 출력 최종 !!!
print("Number of images in content/data after FER-2013 merge:")
for category in ['anger', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']:
    folder_path = os.path.join(content_data_path, category)
    print(f"{category}: {count_files_in_folder(folder_path)}")

# 폴더와 파일 경로 설정
affectnet_folder = '/content/affectnet'
fer2013_folder = '/content/fer2013'
affectnet_zip = '/content/affectnet.zip'
fer2013_zip = '/content/facial-expression-dataset-image-folders-fer2013.zip'

# 폴더 삭제 함수
def delete_folder(folder_path):
    if os.path.exists(folder_path):
        shutil.rmtree(folder_path)
        print(f"Deleted folder: {folder_path}")

# 파일 삭제 함수
def delete_file(file_path):
    if os.path.exists(file_path):
        os.remove(file_path)
        print(f"Deleted file: {file_path}")

# 폴더 삭제
delete_folder(affectnet_folder)
delete_folder(fer2013_folder)

# 파일 삭제
delete_file(affectnet_zip)
delete_file(fer2013_zip)

print("Cleanup complete.")

from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader, random_split
from torchvision import transforms

def process_image(image):
    processed_image = vit_processor(images=image, return_tensors="pt").pixel_values
    return processed_image.squeeze(0)

import os
import shutil
from sklearn.model_selection import train_test_split

# 원본 데이터 경로
data_dir = '/content/data'
# train, val, test 폴더를 생성할 기본 경로
output_dir = '/content/split_data'

train_ratio = 0.7
val_ratio = 0.2
test_ratio = 0.1

# 클래스 폴더 리스트
class_folders = ['anger', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']

# train, val, test 폴더 생성
for folder in ['train', 'val', 'test']:
    for class_folder in class_folders:
        os.makedirs(os.path.join(output_dir, folder, class_folder), exist_ok=True)


for class_folder in class_folders:
    # 원본 데이터의 클래스별 이미지 파일 목록 가져오기
    class_path = os.path.join(data_dir, class_folder)
    images = os.listdir(class_path)

    # train, test split
    train_images, temp_images = train_test_split(images, test_size=(val_ratio + test_ratio), random_state=42)
    # validation, test split
    val_images, test_images = train_test_split(temp_images, test_size=(test_ratio / (val_ratio + test_ratio)), random_state=42)

    # 이미지 파일을 각 폴더로 복사
    for image in train_images:
        src = os.path.join(class_path, image)
        dst = os.path.join(output_dir, 'train', class_folder, image)
        shutil.copy(src, dst)

    for image in val_images:
        src = os.path.join(class_path, image)
        dst = os.path.join(output_dir, 'val', class_folder, image)
        shutil.copy(src, dst)

    for image in test_images:
        src = os.path.join(class_path, image)
        dst = os.path.join(output_dir, 'test', class_folder, image)
        shutil.copy(src, dst)

print("Dataset split completed!")

# 기존 폴더 지저분하니 삭제
delete_folder('/content/data')

### ???
# !rm -R /content/drive/MyDrive/split_dataset/train.ipynb_checkpoints
# !rm -R /content/drive/MyDrive/split_dataset/val.ipynb_checkpoints
# !rm -R /content/drive/MyDrive/split_dataset/test.ipynb_checkpoints

size = (224, 224)        # 확인 필요

train_data_augmentation = transforms.Compose([
    transforms.Lambda(process_image),
    transforms.RandomResizedCrop(size),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(degrees=2),
    # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])

val_data_augmentation = transforms.Compose([
    transforms.Lambda(process_image),
    transforms.CenterCrop(size),
    # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Rescaling(scale=1.0 / 127.5, offset=-1) 적용  ...?
])

train_dir = "/content/split_data/train"
val_dir = "/content/split_data/val"
test_dir = "/content/split_data/test"

train_dataset = ImageFolder(
    root=train_dir,
    transform=train_data_augmentation
)
val_dataset = ImageFolder(
    root=val_dir,
    transform=val_data_augmentation
)
test_dataset = ImageFolder(
    root=test_dir,
    transform=transforms.Lambda(process_image)
)

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)

"""# Model & Loss 정의"""

def _get_vector_norm(tensor: torch.Tensor) -> torch.Tensor:
        square_tensor = torch.pow(tensor, 2)
        sum_tensor = torch.sum(square_tensor, dim=-1, keepdim=True)
        normed_tensor = torch.pow(sum_tensor, 0.5)
        return normed_tensor

import torch
import torch.nn as nn
import torch.optim as optim

class VisionTextModel(nn.Module):
    def __init__(self, vision_model, text_model, visual_projection_dim, text_projection_dim, logit_scale=2.6592):
        super(VisionTextModel, self).__init__()
        vision_model.init_weights()
        self.vision_model = vision_model    # vit model은 초기화 해서 사용
        self.text_model = text_model
        self.visual_projection = nn.Linear(vision_model.config.hidden_size, visual_projection_dim)
        self.text_projection = nn.Linear(text_model.config.hidden_size, text_projection_dim)
        self.logit_scale = nn.Parameter(torch.tensor(logit_scale))

    def forward(self, images, texts, return_loss=False):
        # Get embeddings from both models
        vision_outputs = self.vision_model(**images)       #재확인 필요
        text_outputs = self.text_model(**texts)

        # Extract the embeddings
        image_embeds = vision_outputs.pooler_output
        text_embeds = text_outputs.pooler_output

        # Project embeddings to a common space
        image_embeds = self.visual_projection(image_embeds)
        text_embeds = self.text_projection(text_embeds)

        # Normalize the embeddings
        image_embeds = image_embeds / _get_vector_norm(image_embeds)
        text_embeds = text_embeds / _get_vector_norm(text_embeds)

        # Calculate cosine similarity as logits
        logit_scale = self.logit_scale.exp()
        logits_per_text = torch.matmul(text_embeds, image_embeds.t().to(text_embeds.device)) * logit_scale.to(text_embeds.device)
        logits_per_image = logits_per_text.t()

        return logits_per_text

import torch.nn.functional as F

class ContrastiveLoss(nn.Module):
    def __init__(self):
        super(ContrastiveLoss, self).__init__()

    def contrastive_loss(self, logits: torch.Tensor) -> torch.Tensor:
        return F.cross_entropy(logits, torch.arange(len(logits), device=logits.device))

    def forward(self, logits: torch.Tensor) -> torch.Tensor:
        text_loss = self.contrastive_loss(logits)
        image_loss = self.contrastive_loss(logits.t())

        average_loss = (text_loss + image_loss) / 2.0

        return average_loss

model = VisionTextModel(vit_model, kobert_model, visual_projection_dim=512, text_projection_dim=512)
criterion = ContrastiveLoss()
optimizer = optim.Adam(model.parameters(), lr=0.005)

model_output = model(image_inputs, text_inputs)
model_output.shape    # N x N similarity matrix

"""# Training

"""

from torch.optim.lr_scheduler import StepLR

scheduler = StepLR(optimizer, step_size=10, gamma=0.1)

epochs = 100

train_losses = []
val_losses = []

for epoch in range(epochs):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for inputs, labels in train_loader:
        inputs = inputs.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * labels.size(0)
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
    scheduler.step()

    train_loss = running_loss / total
    train_losses.append(train_loss)
    train_accuracy = correct / total

    model.eval()
    val_loss = 0.0
    val_correct = 0
    val_total = 0
    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            outputs = model(inputs)
            loss = criterion(outputs, labels)
            val_loss += loss.item() * labels.size(0)
            _, predicted = torch.max(outputs, 1)
            val_total += labels.size(0)
            val_correct += (predicted == labels).sum().item()

    val_loss /= val_total
    val_losses.append(val_loss)
    val_accuracy = val_correct / val_total

    print(f'Epoch {epoch+1}/{epochs}, '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')
