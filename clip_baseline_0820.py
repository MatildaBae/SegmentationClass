# -*- coding: utf-8 -*-
"""CLIP_baseline_0820.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1up9avyXdQQ8sUk7bJKvE9p-ptSNgAMrf
"""

!pip install torch torchvision transformers deeplake

"""## 데이터셋 불러오기"""

import deeplake
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image

# FER-2013 데이터셋 로드
train_ds = deeplake.load('hub://activeloop/fer2013-train')
test_ds = deeplake.load('hub://activeloop/fer2013-public-test')

# Custom Dataset 클래스 정의
class FER2013Dataset(Dataset):
    def __init__(self, ds, transform=None):
        self.ds = ds
        self.transform = transform

    def __len__(self):
        return len(self.ds)

    def __getitem__(self, idx):
        image = Image.fromarray(self.ds[idx]["images"].numpy())
        label = self.ds[idx]["labels"].numpy()

        if self.transform:
            image = self.transform(image)

        return image, label

# 데이터 변환 정의 (흑백 이미지를 RGB로 변환)
transform = transforms.Compose([
    transforms.Grayscale(num_output_channels=3),  # 흑백 이미지를 3채널로 변환
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# 데이터셋 인스턴스 생성
train_dataset = FER2013Dataset(train_ds, transform=transform)
test_dataset = FER2013Dataset(test_ds, transform=transform)

# DataLoader 생성
train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)

"""## 모델 임포트"""

import torch
from torch import nn
from transformers import CLIPModel, CLIPProcessor, AutoTokenizer, AutoModel

# CLIP과 KoBERT 모델 불러오기
clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
kobert_tokenizer = AutoTokenizer.from_pretrained("monologg/kobert")
kobert_model = AutoModel.from_pretrained("monologg/kobert")

"""## 모델 클래스 정의
- CLIP 이미지 인코더와 KoBERT 텍스트 인코더 결합
"""

class CLIPKoBERTModel(nn.Module):
    def __init__(self, clip_model, kobert_model):
        super(CLIPKoBERTModel, self).__init__()
        self.clip_model = clip_model
        self.kobert_model = kobert_model

        # CLIP의 이미지 인코더와 KoBERT의 텍스트 인코더 결합
        self.image_encoder = clip_model.vision_model
        self.text_encoder = kobert_model

        # 이미지와 텍스트 임베딩을 동일한 차원으로 변환
        self.image_projection = nn.Linear(self.image_encoder.config.hidden_size, 512)
        self.text_projection = nn.Linear(self.text_encoder.config.hidden_size, 512)

    def forward(self, images, input_ids, attention_mask):
        # 이미지 임베딩
        image_outputs = self.image_encoder(images).pooler_output
        image_embeddings = self.image_projection(image_outputs)

        # 텍스트 임베딩
        text_outputs = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)
        text_embeddings = self.text_projection(text_outputs.pooler_output)

        # 이미지와 텍스트 임베딩 정규화
        image_embeddings = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True)
        text_embeddings = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)

        # 이미지와 텍스트 간의 유사도 계산
        similarity = image_embeddings @ text_embeddings.T

        return similarity

"""Loss 정의"""

def contrastive_loss(logits: torch.Tensor) -> torch.Tensor: # logit 크기가 target 텐서 범위에 포함되는지 확인
    # logits 크기 및 타겟 레이블 확인
    batch_size, num_classes = logits.size()

    # 타겟 레이블 생성
    target = torch.arange(batch_size, device=logits.device)

    # 타겟 레이블이 유효한 범위 내에 있는지 확인
    if target.max() >= num_classes:
        raise ValueError(f"Target index {target.max()} is out of bounds for logits with {num_classes} classes.")

    return nn.functional.cross_entropy(logits, target)

def clip_loss(similarity: torch.Tensor) -> torch.Tensor:
    caption_loss = contrastive_loss(similarity)
    image_loss = contrastive_loss(similarity.t())
    return (caption_loss + image_loss) / 2.0

def calculate_accuracy(similarity: torch.Tensor) -> float:
    pred = similarity.argmax(dim=1)
    correct = torch.eq(pred, torch.arange(len(pred), device=similarity.device)).sum().item()
    accuracy = correct / len(pred)
    return accuracy

# Device 설정
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 모델 초기화
model = CLIPKoBERTModel(clip_model, kobert_model).to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)

"""세분화된 감정 레이블 및 데이터셋 불러오기"""

# 세분화된 감정 레이블 텍스트 예시
LABELS = ['불평/불만', '환영/호의', '감동/감탄', '지긋지긋', '고마움', '슬픔', '화남/분노', '존경', '기대감', '우쭐댐/무시함',
          '안타까움/실망', '비장함', '의심/불신', '뿌듯함', '편안/쾌적', '신기함/관심', '아껴주는', '부끄러움', '공포/무서움',
          '절망', '한심함', '역겨움/징그러움', '짜증', '어이없음', '없음', '패배/자기혐오', '귀찮음', '힘듦/지침', '즐거움/신남',
          '깨달음', '죄책감', '증오/혐오', '흐뭇함(귀여움/예쁨)', '당황/난처', '경악', '부담/안_내킴', '서러움', '재미없음', '불쌍함/연민', '놀람',
          '행복', '불안/걱정', '기쁨', '안심/신뢰']

# 감정 텍스트를 토큰화
encoded_texts = kobert_tokenizer(LABELS, padding=True, return_tensors="pt")

# 데이터 로더 설정
train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)

# 학습 기록을 위한 리스트
train_losses = []
train_accuracies = []

num_epochs = 10

# 학습 루프
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    running_accuracy = 0.0
    for batch in train_dataloader:
        images = batch[0].to(device)  # batch["images"] 대신 batch[0] 사용
        input_ids = encoded_texts["input_ids"].to(device)
        attention_mask = encoded_texts["attention_mask"].to(device)

        # 모델 예측
        similarity = model(images, input_ids, attention_mask)

        # 손실 계산
        loss = clip_loss(similarity)

        # 정확도 계산
        accuracy = calculate_accuracy(similarity)

        # 역전파 및 최적화
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        running_accuracy += accuracy

    epoch_loss = running_loss / len(train_dataloader)
    epoch_accuracy = running_accuracy / len(train_dataloader)
    train_losses.append(epoch_loss)
    train_accuracies.append(epoch_accuracy)

    print(f"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss}, Accuracy: {epoch_accuracy}")

"""### IndexError: Target 32 is out of bounds. 오류 디버깅"""

# 모델 예측 후 크기 확인
print(f"Similarity tensor size: {similarity.size()}")

# 타겟 레이블 크기 확인
print(f"Target size: {torch.arange(len(similarity), device=similarity.device).size()}")

# 타겟 텐서 확인
target = torch.arange(len(similarity), device=similarity.device)
print(f"Target tensor: {target}")
print(f"Max target value: {target.max()}, Num classes: {similarity.size(1)}")

# 이미지와 텍스트 임베딩의 크기를 확인
image_embeddings = model.image_projection(image_outputs)
text_embeddings = model.text_projection(text_outputs.pooler_output)

print(f"Image Embeddings size: {image_embeddings.size()}")
print(f"Text Embeddings size: {text_embeddings.size()}")

# 유사도 계산 후 크기 확인
similarity = image_embeddings @ text_embeddings.T
print(f"Similarity tensor size: {similarity.size()}")

"""## 결과"""

import matplotlib.pyplot as plt

# 학습 손실 및 정확도 플롯
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(range(num_epochs), train_losses, label='Training Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training Loss Over Time')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(range(num_epochs), train_accuracies, label='Training Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training Accuracy Over Time')
plt.legend()

plt.show()

"""Prediction"""

def predict_emotion(model, image, emotion_labels):
    model.eval()

    with torch.no_grad():
        # 이미지 임베딩 생성
        image_embedding = model.image_encoder(image).pooler_output
        image_embedding = model.image_projection(image_embedding)
        image_embedding = image_embedding / image_embedding.norm(dim=-1, keepdim=True)

        # 감정 텍스트 임베딩 생성
        input_ids = kobert_tokenizer(emotion_labels, return_tensors="pt", padding=True)["input_ids"].to(image.device)
        attention_mask = kobert_tokenizer(emotion_labels, return_tensors="pt", padding=True)["attention_mask"].to(image.device)
        text_embeddings = model.text_encoder(input_ids=input_ids, attention_mask=attention_mask).pooler_output
        text_embeddings = model.text_projection(text_embeddings)
        text_embeddings = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)

        # 유사도 계산
        logits_per_image = image_embedding @ text_embeddings.T
        probs = logits_per_image.softmax(dim=-1)

        # 가장 높은 확률의 감정 레이블 선택
        predicted_emotion = emotion_labels[probs.argmax()]

        return predicted_emotion

# 예측 예시
image = ... # 예측하고자 하는 이미지
predicted_emotion = predict_emotion(model, image, emotion_labels)
print(f"Predicted emotion: {predicted_emotion}")