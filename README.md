# â¤ï¸ Subtle Emotion Recognition Using CLIP Model

**Team SOã…‹TA**  
*Members: Park Se-hoon, Bae Ji-won, Jeong Ha-yeon*  

## ğŸš€ Overview

Welcome to the Emotion Recognition Using CLIP Model project. This project, developed by Team SOã…‹TA, explores the use of a fine-tuned CLIP model to perform **zero-shot prediction** for complex emotions that were not part of the training labels. Our primary goal was to leverage CLIP's strengths in multi-modal tasks to identify emotions in images that map to text-embedded emotions.

We observed that many emotion datasets for facial expressions have limited and overly simplistic emotion labels. Our motivation was to develop a model that could recognize more nuanced emotions and ultimately create a dataset that could serve broader purposes, such as psychological analysis or improving AI-human interactions.

---

## ğŸ’¡ Project Objectives

1. Fine-tune the **CLIP model** for zero-shot emotion recognition.
2. Expand emotion labels beyond simple categories using **image feature embeddings** linked to **text embeddings**.
3. Create a dataset with more granular emotional labels to enhance emotion recognition capabilities.

---

## ğŸ” Methodology

### 1. Data Collection

We used two main datasets:
- **FER-2013**: A dataset containing 35,887 grayscale images labeled with 7 basic emotions (anger, disgust, fear, happiness, sadness, surprise, and neutral).
- **AffectNet**: A dataset with over 1 million images labeled with 8 emotions (same as FER-2013 plus contempt).

We merged these datasets to ensure the availability of a sufficient number of images for each emotion class. The **Kaggle dataset API** was used to download and merge the datasets while ensuring label balance in each batch.

### 2. Model Architecture

We chose the CLIP model due to its ability to connect **text and image embeddings**. For our task, we modified the model architecture by:
- Adding depth to the image encoder and classifier layers.
- Using **Ray Tune** and manual hyperparameter tuning to optimize parameters such as learning rate, logit scale, and embedding size.
- Fine-tuning both text and image encoders to ensure they aligned with our more complex emotion labels.

### 3. Metrics and Approach

To calculate the **affection score**:
1. **Turn-wise Affection Scoring**: Each turn's affection score is computed using the probability of each emotion label and its associated weight.
2. **Contextual Conversation Scoring**: The overall affection score is computed by summing the product of the probabilities and weights of emotion labels across turns, with greater weight given to recent turns (turn number/total turns).

We used **KoBERT word embeddings** to calculate the cosine similarity between emotions like â€˜Excitement,â€™ â€˜Interest,â€™ â€˜Affection,â€™ and â€˜Love,â€™ and applied these as part of our scoring methodology.

---

## ğŸ§‘â€ğŸ’» Training Process

1. **Small Dataset Training**: We initially trained a small dataset of 256 images for 100 epochs, achieving a reduction in loss from 2.7 to 1.2 and an improvement in accuracy from 0.16 to 0.40.
2. **Hyperparameter Tuning**: We manually implemented random search to find the optimal combination of parameters, including learning rate, logit scale, and embedding size.
3. **Model Testing**: Using the KOTE dataset, we aimed to classify more complex emotions like "Confused," "Resigned," and "Dignified," but meaningful results have yet to be achieved.

---

## Limitations and Future Work

- We faced challenges in achieving meaningful accuracy with more complex emotions and are currently exploring ways to improve the model by experimenting with different architectures such as replacing **ViT** with **ResNet**.
- Our next task is to focus on classifying 36 new emotions defined in the Korean language corpus and calculating **top-5 accuracy** for these categories.

---

## Conclusion

While we faced several challenges in tuning the CLIP model for more complex emotions, this project laid the groundwork for future research. We aim to continue refining the model and extend its application to diverse fields such as psychological analysis and interactive AI systems.

---

## Acknowledgments

- This project was created by Team SOã…‹TA: **Park Se-hoon, Bae Ji-won, Jeong Ha-yeon**.

---

# â¤ï¸ CLIP ëª¨ë¸ì„ í™œìš©í•œ ê°ì • ì¸ì‹

**Team SOã…‹TA**  
*íŒ€ì›: ë°•ì„¸í›ˆ, ë°°ì§€ì›, ì •í•˜ì—°*

## ğŸš€ ê°œìš”

'CLIP ëª¨ë¸ì„ í™œìš©í•œ ê°ì • ì¸ì‹' í”„ë¡œì íŠ¸ ë¦¬í¬ì§€í† ë¦¬ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤. ì´ í”„ë¡œì íŠ¸ëŠ” Team SOã…‹TAê°€ ê°œë°œí•œ ê²ƒìœ¼ë¡œ, **zero-shot prediction**ì„ í†µí•´ í•™ìŠµë˜ì§€ ì•Šì€ ë³µì¡í•œ ê°ì •ì„ ì¸ì‹í•˜ëŠ” ëª¨ë¸ì„ ëª©í‘œë¡œ í–ˆìŠµë‹ˆë‹¤. ì €í¬ëŠ” ë©€í‹°ëª¨ë‹¬ íƒœìŠ¤í¬ì— ê°•ì ì´ ìˆëŠ” CLIP ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ì—¬ í…ìŠ¤íŠ¸ ì„ë² ë”©ê³¼ ì´ë¯¸ì§€ í”¼ì³ ì„ë² ë”©ì„ ì—°ê²°í•˜ëŠ” ëª¨ë¸ì„ ê°œë°œí•˜ê³ ì í–ˆìŠµë‹ˆë‹¤.

ì €í¬ëŠ” ê¸°ì¡´ í‘œì • ë°ì´í„°ì…‹ì—ì„œ ì‚¬ìš©ë˜ëŠ” ê°ì • ë ˆì´ë¸”ì´ ë‹¨ìˆœí•˜ë‹¤ëŠ” ë¬¸ì œë¥¼ ë°œê²¬í•˜ê³ , ë” ë¯¸ë¬˜í•œ ê°ì •ì„ ì¸ì‹í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ ê°œë°œí•˜ê³ ì í–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì„¸ë¶„í™”ëœ ê°ì • ë ˆì´ë¸”ì„ í™œìš©í•œ ë°ì´í„°ì…‹ì„ ë§Œë“¤ì–´ ì‹¬ë¦¬ ë¶„ì„ ë° AI-ì¸ê°„ ìƒí˜¸ì‘ìš©ì— ê¸°ì—¬í•˜ê³ ì í–ˆìŠµë‹ˆë‹¤.

---

## ğŸ’¡ í”„ë¡œì íŠ¸ ëª©í‘œ

1. **CLIP ëª¨ë¸**ì„ íŒŒì¸íŠœë‹í•˜ì—¬ zero-shot ê°ì • ì¸ì‹ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
2. ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ê°„ ì„ë² ë”©ì„ ì—°ê²°í•˜ì—¬ ë” ë³µì¡í•œ ê°ì • ë ˆì´ë¸”ì„ êµ¬ë¶„í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.
3. ë” ì„¸ë¶„í™”ëœ ê°ì • ë°ì´í„°ì…‹ì„ ë§Œë“¤ì–´ ë°°í¬í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.

---

## ğŸ” ë°©ë²•ë¡ 

### 1. ë°ì´í„° ìˆ˜ì§‘

ì €í¬ëŠ” ë‘ ê°€ì§€ ì£¼ìš” ë°ì´í„°ì…‹ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤:
- **FER-2013**: 35,887ê°œì˜ í‘ë°± ì´ë¯¸ì§€ë¡œ êµ¬ì„±ëœ ë°ì´í„°ì…‹ìœ¼ë¡œ, 7ê°€ì§€ ê¸°ë³¸ ê°ì •(í™”ë‚¨, í˜ì˜¤, ë‘ë ¤ì›€, í–‰ë³µ, ìŠ¬í””, ë†€ëŒ, ì¤‘ë¦½)ìœ¼ë¡œ ë¼ë²¨ë§ë¨.
- **AffectNet**: 1ë°±ë§Œ ê°œ ì´ìƒì˜ ì´ë¯¸ì§€ë¡œ êµ¬ì„±ëœ ë°ì´í„°ì…‹ìœ¼ë¡œ, 8ê°€ì§€ ê°ì •(ê¸°ë³¸ ê°ì • + ê²½ë©¸)ìœ¼ë¡œ ë¼ë²¨ë§ë¨.

ì €í¬ëŠ” ì´ ë°ì´í„°ì…‹ì„ í•©ì³ì„œ ì¶©ë¶„í•œ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ í™•ë³´í•˜ê³ , **Kaggle dataset API**ë¥¼ ì´ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ í†µí•©í•˜ê³  ë°°ì¹˜ë§ˆë‹¤ ë¼ë²¨ì˜ ê· í˜•ì„ ë§ì·„ìŠµë‹ˆë‹¤.

### 2. ëª¨ë¸ ì•„í‚¤í…ì³

ì €í¬ëŠ” CLIP ëª¨ë¸ì„ ì„ íƒí•˜ì˜€ìœ¼ë©°, í…ìŠ¤íŠ¸ ì„ë² ë”©ê³¼ ì´ë¯¸ì§€ í”¼ì³ ê°„ì˜ ì—°ê²°ì„ ì‹œë„í–ˆìŠµë‹ˆë‹¤. 
- ì´ë¯¸ì§€ ì¸ì½”ë”ì™€ ë¶„ë¥˜ê¸°ì˜ ë ˆì´ì–´ ê¹Šì´ë¥¼ ì¶”ê°€.
- **Ray Tune** ë° ìˆ˜ë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµë¥ , ë¡œì§“ ìŠ¤ì¼€ì¼, ì„ë² ë”© í¬ê¸° ë“± ìµœì ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.
- ë” ë³µì¡í•œ ê°ì • ë¼ë²¨ì„ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ì¸ì½”ë” ëª¨ë‘ë¥¼ íŒŒì¸íŠœë‹í–ˆìŠµë‹ˆë‹¤.

### 3. ë©”íŠ¸ë¦­ ë° ì ‘ê·¼ ë°©ì‹

**í˜¸ê°ë„ ì ìˆ˜**ë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
1. **í„´ë³„ í˜¸ê°ë„ ì ìˆ˜ ì‚°ì¶œ**: ê° í„´ì˜ í˜¸ê°ë„ ì ìˆ˜ëŠ” ê°ì • ë ˆì´ë¸”ì˜ í™•ë¥ ê³¼ ê°€ì¤‘ì¹˜ë¥¼ ê³±í•˜ì—¬ ê³„ì‚°.
2. **ë§¥ë½ì„ ê³ ë ¤í•œ ëŒ€í™” ë¶„ìœ„ê¸° íŒŒì•…**: ëŒ€í™”ì˜ ì „ì²´ ë¶„ìœ„ê¸°ëŠ” ê°ì • ë ˆì´ë¸” í™•ë¥ ê³¼ ê°€ì¤‘ì¹˜ë¥¼ ë”í•œ í›„, ìµœì‹  í„´ì— ë” í° ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ì—¬ ê³„ì‚°.

ì €í¬ëŠ” **KoBERT ì›Œë“œ ì„ë² ë”©**ì„ ì‚¬ìš©í•˜ì—¬ â€˜ì„¤ë ˜â€™, â€˜ê´€ì‹¬â€™, â€˜í˜¸ê°â€™, â€˜ì‚¬ë‘â€™ ë“± ê°ì • ë¼ë²¨ ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ì˜€ìŠµë‹ˆë‹¤.

---

## ğŸ§‘â€ğŸ’» í•™ìŠµ ê³¼ì •

1. **ì†Œê·œëª¨ ë°ì´í„°ì…‹ í•™ìŠµ**: 256ê°œì˜ ì´ë¯¸ì§€ë¡œ êµ¬ì„±ëœ small datasetìœ¼ë¡œ 100 epochë¥¼ í•™ìŠµí•˜ì—¬, lossê°€ 2.7ì—ì„œ 1.2ë¡œ ê°ì†Œí•˜ê³ , accuracyê°€ 0.16ì—ì„œ 0.40ê¹Œì§€ ì¦ê°€í•˜ëŠ” ê²ƒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.
2. **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**: í•™ìŠµë¥ , ë¡œì§“ ìŠ¤ì¼€ì¼, ì„ë² ë”© í¬ê¸°ë¥¼ ìµœì í™”í•˜ê¸° ìœ„í•´ random searchë¥¼ ìˆ˜ë™ìœ¼ë¡œ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.
3. **ëª¨ë¸ í…ŒìŠ¤íŠ¸**: **KOTE ë°ì´í„°ì…‹**ì„ ì‚¬ìš©í•˜ì—¬ ë” ë³µì¡í•œ ê°ì •(ì˜ˆ: "ë‹¹í™©", "í•œì‹¬", "ë¹„ì¥í•¨")ì„ ë¶„ë¥˜í•˜ë ¤ í–ˆì§€ë§Œ, ì•„ì§ ìœ ì˜ë¯¸í•œ ê²°ê³¼ëŠ” ë„ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.

---

## í•œê³„ì  ë° í–¥í›„ ì‘ì—…

- ë” ë³µì¡í•œ ê°ì •ì„ í•™ìŠµí•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªì—ˆê³ , **ViT** ëŒ€ì‹  **ResNet** ë“± ë‹¤ë¥¸ ì•„í‚¤í…ì²˜ë¥¼ ì‹¤í—˜í•˜ê³  ìˆìŠµë‹ˆë‹¤.
- í•œêµ­ì–´ ë§ë­‰ì¹˜ ë°ì´í„°ì…‹ì—ì„œ ì •ì˜ëœ 36ê°œì˜ ìƒˆë¡œìš´ ê°ì •ì„ ë¶„ë¥˜í•˜ê³  **top-5 accuracy**ë¥¼ ê³„ì‚°í•˜ëŠ” ê²ƒì´ ì €í¬ì˜ ë‹¤ìŒ ëª©í‘œì…ë‹ˆë‹¤.

---

## ê²°ë¡ 

ì´ë²ˆ í”„ë¡œì íŠ¸ëŠ” ë³µì¡í•œ ê°ì • ì¸ì‹ì„ ìœ„í•œ CLIP ëª¨ë¸ íŠœë‹ì˜ ê¸°ì´ˆë¥¼ ë§ˆë ¨í–ˆìŠµë‹ˆë‹¤. í–¥í›„ ì‹¬ë¦¬ ë¶„ì„, AI-ì¸ê°„ ìƒí˜¸ì‘ìš© ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì— ì´ ëª¨ë¸ì„ í™•ì¥í•´ ë‚˜ê°ˆ ê³„íšì…ë‹ˆë‹¤.

---

## ê°ì‚¬ì˜ ë§

- ì´ í”„ë¡œì íŠ¸ëŠ” Team SOã…‹TA: **ë°•ì„¸í›ˆ, ë°°ì§€ì›, ì •í•˜ì—°**ì´ ì œì‘í•˜ì˜€ìŠµë‹ˆë‹¤.

